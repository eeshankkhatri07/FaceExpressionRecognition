# -*- coding: utf-8 -*-
"""FER2013using .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BaMrDSytJ-aUB7H3p8m9xHGbV8Rxdv9l
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from __future__ import print_function
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten
from keras.utils import to_categorical
import pandas as pd
from google.colab import drive
from sklearn.model_selection import train_test_split

# Mount Google Drive
drive.mount('/content/drive')

# Load dataset from Google Drive
data_path = '/content/drive/My Drive/fer2013.csv'  # Change this to the actual path
data = pd.read_csv(data_path)

def process_data(data):
    X = []
    y = []
    for i in range(len(data)):
        img = np.fromstring(data['pixels'][i], dtype=int, sep=' ').reshape(48, 48)
        X.append(img)
        y.append(data['emotion'][i])
    X = np.array(X).reshape(-1, 48, 48, 1)
    y = to_categorical(y, num_classes=len(data['emotion'].unique()))  # Dynamically adjust class count
    return X, y

X, y = process_data(data)
X = X / 255.0  # Normalize pixel values

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model architecture

# Model architecture
model = Sequential([
    Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.3),

    Conv2D(128, kernel_size=(3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.3),

    Flatten(),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),

    Dense(len(data['emotion'].unique()), activation='softmax')  # Dynamically adjust class count
])

model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train,
                    batch_size=64,
                    epochs=25,
                    validation_data=(X_test, y_test),
                    verbose=1)

# Evaluate model
loss, acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {acc * 100:.2f}%")

# Save model to Google Drive
model_path = '/content/drive/My Drive/face_expression_model.h5'
model.save(model_path)
print(f"Model saved at {model_path}")

# Plot accuracy
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.legend()
plt.show()

